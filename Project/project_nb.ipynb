{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4da38ad",
   "metadata": {},
   "source": [
    "# 1. Libraries & Sample Data\n",
    "The first step is to load our Python Libraries and download the sample data. The dataset represents Apple stock price (1d bars) for the year 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa05430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-configure CUDA paths for Windows if installed via pip/uv\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    try:\n",
    "        import nvidia.cudnn.lib\n",
    "        import nvidia.cublas.lib\n",
    "        \n",
    "        cudnn_dir = os.path.dirname(nvidia.cudnn.lib.__file__)\n",
    "        cublas_dir = os.path.dirname(nvidia.cublas.lib.__file__)\n",
    "        \n",
    "        # Add to PATH\n",
    "        os.environ['PATH'] = cudnn_dir + os.pathsep + cublas_dir + os.pathsep + os.environ['PATH']\n",
    "        \n",
    "        # Also, we might need 'bin' dirs for some packages if they structure it that way\n",
    "        # But nvidia-cudnn-cu11 on pypi usually puts dlls in the package dir or lib dir.\n",
    "        print(f\"Added CUDA paths: {cudnn_dir}\")\n",
    "    except ImportError:\n",
    "        print(\"NVIDIA python packages not found, falling back to system PATH for CUDA.\")\n",
    "\n",
    "\n",
    "# Load Python Libraries\n",
    "import math\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for dataframe display\n",
    "pd.set_option('display.max_rows', None)\n",
    "def display_df(df):\n",
    "    # Puts the scrollbar next to the DataFrame\n",
    "    display(HTML(\"<div style='height: 200px; overflow: auto; width: fit-content'>\" + df.to_html() + \"</div>\"))\n",
    "\n",
    "# for reproducability of training rounds\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa831031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Sample Data\n",
    "data = pd.read_csv('GOOG_2009-2010_6m_RAW_1d.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a4a35",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "Next, we want to analyze our data. Display the data as a dataframe, and plot some relevant data so you can get an idea of what our dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display as Dataframe\n",
    "display_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Close Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bc851",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "Next, we need to clean our data for training our model. This requires removal of NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ebfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9128668",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6050c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(data['Close'])\n",
    "plt.title('Cleaned Close Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764fed7f",
   "metadata": {},
   "source": [
    "# 4. Feature Selection\n",
    "Now that we have cleaned our stock data, we need to select which features to train our model on. For this project, we will be training with Close data and 20-day Bollinger Bands of Close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb99ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 20-day bollinger bands\n",
    "data['MA20'] = data['Close'].rolling(window=20).mean()\n",
    "data['20dSTD'] = data['Close'].rolling(window=20).std()\n",
    "data['Upper'] = data['MA20'] + (data['20dSTD'] * 2)\n",
    "data['Lower'] = data['MA20'] - (data['20dSTD'] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42224d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN bollinger bands\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f48c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new dataframe with only the training features (Close, Upper BB, Lower BB)\n",
    "dataset = data[['Close', 'Upper', 'Lower']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f718a0",
   "metadata": {},
   "source": [
    "# 5. Normalization\n",
    "Now that we have cleaned our data, created our indicators of interest, and selected our features, we must normalize our data. For this project, we use the sklearn StandardScaler, which centers the data and normalizes to unit variance. We will not be using a rolling scaler for this project, due to the complexity of back-translating to true proce and indicator values - you can try this yourself once you have completed the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display & Plot Un-normalized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Dataset with StandardScaler\n",
    "normlist = []\n",
    "normed_dataset = pd.DataFrame(index=dataset.index)\n",
    "for col in dataset.columns:\n",
    "    normalizer = StandardScaler()\n",
    "    column_data = dataset[[col]].values \n",
    "    normalizer.fit(column_data)\n",
    "    normed_dataset[col] = normalizer.transform(column_data).flatten()\n",
    "    normlist.append(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcab016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display & Plot Normalized Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8526a",
   "metadata": {},
   "source": [
    "# 6. Train / Test Split\n",
    "Now that our data cleaned, features are selected, and the dataset is normalized, we are ready to feed the data into our model. With this in mind, we split the data ito train and test data (50/50 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7557879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset df into train (50%) and test (50%) datasets\n",
    "l = len(normed_dataset)\n",
    "train_len = int(l/2)\n",
    "\n",
    "train_df = normed_dataset.iloc[:train_len]\n",
    "test_df = normed_dataset.iloc[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dca49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train and test dfs (ensure no overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train and test dfs to np arrays with dtype=float\n",
    "X_train = train_df.values.astype(float)\n",
    "X_test = test_df.values.astype(float)\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "\n",
    "idx_close = 0 # numerical idx of close data column in array\n",
    "idx_bb_upper = 1 # numerical idx of BB Upper data column in array\n",
    "idx_bb_lower = 2 # numerical idx of BB Lower data column in array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c77a8",
   "metadata": {},
   "source": [
    "# 7. Define the Agent\n",
    "Now that our data is ready to use, we can define the Reinforcement Learning Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd06b6d",
   "metadata": {},
   "source": [
    "### Define the DQN Model\n",
    "The first step in defining our agent is the Deep Q-Network model definition. For this project, we are creating a model sequential model with four layers. The first three layers have output shape of 64, 32, and 8, respectively, and a RELU activation. The output layer has an output shape of the size of our action space (buy, sell, hold), and a linear activation. Our Loss finction is Mean Squared Error, and our optimizer is Adam with a learning rate of 0.001. Use Keras to build this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e98591",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.utils.register_keras_serializable()\n",
    "# Define DQN Model Architecture\n",
    "class DQN(keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        # define model layers in keras\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Input(shape=(state_size,)),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(32, activation='relu'),\n",
    "            keras.layers.Dense(8, activation='relu'),\n",
    "            keras.layers.Dense(action_size, activation='linear')\n",
    "        ])\n",
    "        # compile model in keras\n",
    "        self.model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edabcd7d",
   "metadata": {},
   "source": [
    "### Define Agent Class\n",
    "Now that we have defined our underlying DQN Model, we must define out Reinforcement Learning Agent. The agent initialization is provided for you, you must define an act function, and an expereince replay function. As a reminder, the act function defines how our model will act (buy, hold, or sell) given a certain state. The Experience Replay function tackles catastrophic forgetting in our training process, by maintaining a memory buffer to allow training on independent / randomized minibatches of previous states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1489f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, window_size, num_features, test_mode=False, model_name=''):\n",
    "        self.window_size = window_size \n",
    "        self.num_features = num_features \n",
    "        self.state_size = (window_size + 1) * num_features # fixed to include window history + current\n",
    "        self.action_size = 3 # 0=hold, 1=buy, 2=sell\n",
    "        self.memory = deque(maxlen=1000) \n",
    "        self.inventory = [] \n",
    "        self.model_name = model_name \n",
    "        self.test_mode = test_mode \n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        self.model = keras.models.load_model(model_name) if test_mode else self._model()\n",
    "\n",
    "    def _model(self):\n",
    "        model = DQN(self.state_size, self.action_size).model\n",
    "        return model\n",
    "    \n",
    "    def get_q_values_for_state(self, state):\n",
    "        return self.model.predict(state.flatten().reshape(1, self.state_size), verbose=0)\n",
    "    \n",
    "    def fit_model(self, input_state, target_output):\n",
    "        return self.model.fit(input_state.flatten().reshape(1, self.state_size), target_output, epochs=1, verbose=0)    \n",
    "    \n",
    "    def act(self, state): \n",
    "        if not self.test_mode and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.get_q_values_for_state(state)\n",
    "        return np.argmax(act_values[0])\n",
    " \n",
    "    def exp_replay(self, batch_size, losses):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return losses\n",
    "        \n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            if done:\n",
    "                target = reward  \n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(self.get_q_values_for_state(next_state)[0])\n",
    "            \n",
    "            target_f = self.get_q_values_for_state(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            history = self.fit_model(state, target_f)\n",
    "            losses += history.history['loss']\n",
    "           \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee09a6",
   "metadata": {},
   "source": [
    "# 8. Train the Agent\n",
    "Now that our data is ready and our agent is defined, we are ready to train the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cd0f4",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Before we define the training loop, we will write some helper functions: one for printing price data, one to define the sigmoid funtion, one to grab the state representation,  one to plot the trading output of our trained model, and one to plot the training loss. The printing, sigmoid, and plotting functions are defined for you. You must define the function which gets the state representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format price string\n",
    "def format_price(n):\n",
    "    return ('-$' if n < 0 else '$') + '{0:.2f}'.format(abs(n))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Plot behavior of trade output\n",
    "def plot_behavior(data_input, bb_upper_data, bb_lower_data, states_buy, states_sell, profit, train=True):\n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    plt.plot(data_input, color='k', lw=2., label= 'Close Price')\n",
    "    plt.plot(bb_upper_data, color='b', lw=2., label = 'Bollinger Bands')\n",
    "    plt.plot(bb_lower_data, color='b', lw=2.)\n",
    "    plt.plot(data_input, '^', markersize=10, color='r', label = 'Buying signal', markevery = states_buy)\n",
    "    plt.plot(data_input, 'v', markersize=10, color='g', label = 'Selling signal', markevery = states_sell)\n",
    "    plt.title('Total gains: %f'%(profit))\n",
    "    plt.legend()\n",
    "    if train:\n",
    "        # Simplified xticks for performance\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    plt.show()\n",
    "\n",
    "def plot_losses(losses, title):\n",
    "    plt.plot(losses)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('MSE Loss Value')\n",
    "    plt.xlabel('batch')\n",
    "    plt.show()\n",
    "\n",
    "def get_state(data, t, n): \n",
    "    # n is state size in days (window_size + 1)\n",
    "    d = t - n + 1\n",
    "    if d >= 0:\n",
    "        window = data[d:t + 1]\n",
    "    else:\n",
    "        # Pad with first row\n",
    "        padding = np.array([data[0]] * (-d))\n",
    "        window = np.concatenate((padding, data[0:t+1]))\n",
    "        \n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        for feature in window[i]:\n",
    "            res.append(sigmoid(feature))\n",
    "    return np.array([res])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6bd8f6",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798817f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the shape of your training data in order to remond yourself how may features and examples there are in your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.disable_interactive_logging() \n",
    "window_size = 1\n",
    "agent = Agent(window_size, num_features=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851650c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.config.disable_traceback_filtering()\n",
    "\n",
    "l = len(X_train) - 1\n",
    "batch_size = 32\n",
    "episode_count = 2\n",
    "\n",
    "normalizer_close = normlist[idx_close]\n",
    "normalizer_bb_upper = normlist[idx_bb_upper]\n",
    "normalizer_bb_lower = normlist[idx_bb_lower]\n",
    "\n",
    "X_train_true_price = normalizer_close.inverse_transform(X_train[:, idx_close].reshape(-1, 1)).flatten()\n",
    "X_train_true_bb_upper = normalizer_bb_upper.inverse_transform(X_train[:, idx_bb_upper].reshape(-1, 1)).flatten()\n",
    "X_train_true_bb_lower = normalizer_bb_lower.inverse_transform(X_train[:, idx_bb_lower].reshape(-1, 1)).flatten()\n",
    "\n",
    "batch_losses = []\n",
    "num_batches_trained = 0\n",
    "\n",
    "for e in range(episode_count + 1):\n",
    "    state = get_state(X_train, 0, window_size + 1)\n",
    "    total_profit = 0\n",
    "    total_winners = 0\n",
    "    total_losers = 0\n",
    "    agent.inventory = []\n",
    "    states_sell = []\n",
    "    states_buy = []\n",
    "    \n",
    "    for t in tqdm(range(l), desc=f'Running episode {e}/{episode_count}'):\n",
    "        action = agent.act(state)\n",
    "        next_state = get_state(X_train, t + 1, window_size + 1)\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1: # buy\n",
    "            buy_price = X_train_true_price[t]\n",
    "            agent.inventory.append(buy_price)\n",
    "            states_buy.append(t)\n",
    "            # print(f\"Buy: {format_price(buy_price)}\")\n",
    "\n",
    "        elif action == 2 and len(agent.inventory) > 0: # sell\n",
    "            bought_price = agent.inventory.pop(0)  \n",
    "            sell_price = X_train_true_price[t]\n",
    "            trade_profit = sell_price - bought_price\n",
    "            reward = max(trade_profit, 0)\n",
    "            total_profit += trade_profit\n",
    "            \n",
    "            if trade_profit >= 0:\n",
    "                total_winners += trade_profit\n",
    "            else:\n",
    "                total_losers += abs(trade_profit)\n",
    "            states_sell.append(t)\n",
    "            # print(f\"Sell: {format_price(sell_price)} | Profit: {format_price(trade_profit)}\")\n",
    "\n",
    "        done = True if t == l - 1 else False\n",
    "        agent.memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print('--------------------------------')\n",
    "            print(f'Episode {e}')\n",
    "            print(f'Total Profit: {format_price(total_profit)}')\n",
    "            print(f'Total Winners: {format_price(total_winners)}')\n",
    "            print(f'Total Losers: {format_price(total_losers)}')\n",
    "            print('--------------------------------')\n",
    "            plot_behavior(X_train_true_price, X_train_true_bb_upper, X_train_true_bb_lower, states_buy, states_sell, total_profit)\n",
    "            plot_losses(batch_losses, f'Episode {e} DQN model loss')\n",
    "            \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.exp_replay(batch_size, batch_losses)\n",
    "            \n",
    "    if e % 10 == 0:\n",
    "        agent.model.save(f'model_ep{e}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599ee50",
   "metadata": {},
   "source": [
    "### Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a03b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the plot_losses function to plot all batch_losses for the entire training round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb831d",
   "metadata": {},
   "source": [
    "# 9. Test the trained agent \n",
    "Finally, we get to test our trained model to see how well it performs in our test set. Using the training loop above, define a method to run our trained model on our X_test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd7885",
   "metadata": {},
   "source": [
    "### Define Parameters\n",
    "Some test parameters are defined for you below. Fill out the missing data. If you need a hint, look up at the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test = len(X_test) - 1\n",
    "state = get_state(X_test, 0, window_size + 1)\n",
    "total_profit = 0\n",
    "done = False\n",
    "states_sell_test = []\n",
    "states_buy_test = []\n",
    "\n",
    "#Get the trained model\n",
    "agent = Agent(window_size, num_features=X_test.shape[1], test_mode=True, model_name=f'model_ep{episode_count}.keras')\n",
    "agent.inventory = []\n",
    "\n",
    "state = get_state(X_test, 0, window_size + 1)\n",
    "\n",
    "X_test_true_price = normalizer_close.inverse_transform(X_test[:, idx_close].reshape(-1, 1)).flatten()\n",
    "X_test_true_bb_upper = normalizer_bb_upper.inverse_transform(X_test[:, idx_bb_upper].reshape(-1, 1)).flatten()\n",
    "X_test_true_bb_lower = normalizer_bb_lower.inverse_transform(X_test[:, idx_bb_lower].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8871f51",
   "metadata": {},
   "source": [
    "### Run the Test\n",
    "Run the test data through the trained model. Look at the training loop for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(l_test):\n",
    "    action = agent.act(state)\n",
    "    next_state = get_state(X_test, t + 1, window_size + 1)\n",
    "    reward = 0\n",
    "\n",
    "    if action == 1: # buy\n",
    "        buy_price = X_test_true_price[t]\n",
    "        agent.inventory.append(buy_price)\n",
    "        states_buy_test.append(t)\n",
    "        print(f'Buy: {format_price(buy_price)}')\n",
    "\n",
    "    elif action == 2 and len(agent.inventory) > 0: # sell\n",
    "        bought_price = agent.inventory.pop(0)\n",
    "        sell_price = X_test_true_price[t]\n",
    "        trade_profit = sell_price - bought_price\n",
    "        total_profit += trade_profit\n",
    "        states_sell_test.append(t)\n",
    "        print(f'Sell: {format_price(sell_price)} | Profit: {format_price(trade_profit)}')\n",
    "\n",
    "    if t == l_test - 1:\n",
    "        done = True\n",
    "    \n",
    "    agent.memory.append((state, action, reward, next_state, done))\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        print('------------------------------------------')\n",
    "        print(f'Total Profit: {format_price(total_profit)}')\n",
    "        print('------------------------------------------')\n",
    "        \n",
    "plot_behavior(X_test_true_price, X_test_true_bb_upper, X_test_true_bb_lower, states_buy_test, states_sell_test, total_profit, train=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
